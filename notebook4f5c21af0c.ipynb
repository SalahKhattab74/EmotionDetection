{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-29T11:12:42.006054Z","iopub.execute_input":"2022-01-29T11:12:42.006372Z","iopub.status.idle":"2022-01-29T11:12:42.040085Z","shell.execute_reply.started":"2022-01-29T11:12:42.006338Z","shell.execute_reply":"2022-01-29T11:12:42.039449Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy\nimport pandas as pd\ndf = pd.read_csv('../input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv')\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:12:42.941637Z","iopub.execute_input":"2022-01-29T11:12:42.942211Z","iopub.status.idle":"2022-01-29T11:12:45.391636Z","shell.execute_reply.started":"2022-01-29T11:12:42.942171Z","shell.execute_reply":"2022-01-29T11:12:45.390958Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"X_train = []\ny_train = []\nX_test = []\ny_test = []\nfor index, row in df.iterrows():\n    k = row['pixels'].split(\" \")\n    if row['Usage'] == 'Training':\n        X_train.append(np.array(k))\n        y_train.append(row['emotion'])\n    elif row['Usage'] == 'PublicTest':\n        X_test.append(np.array(k))\n        y_test.append(row['emotion'])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:12:46.576885Z","iopub.execute_input":"2022-01-29T11:12:46.577565Z","iopub.status.idle":"2022-01-29T11:13:10.793063Z","shell.execute_reply.started":"2022-01-29T11:12:46.577526Z","shell.execute_reply":"2022-01-29T11:13:10.792183Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_train = np.array(X_train, dtype = 'uint8')\ny_train = np.array(y_train, dtype = 'uint8')\nX_test = np.array(X_test, dtype = 'uint8')\ny_test = np.array(y_test, dtype = 'uint8')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:13:10.794764Z","iopub.execute_input":"2022-01-29T11:13:10.795032Z","iopub.status.idle":"2022-01-29T11:13:49.359067Z","shell.execute_reply.started":"2022-01-29T11:13:10.794994Z","shell.execute_reply":"2022-01-29T11:13:49.358157Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom tensorflow.keras.utils import to_categorical\ny_train= to_categorical(y_train, num_classes=7)\ny_test = to_categorical(y_test, num_classes=7)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:14:15.909007Z","iopub.execute_input":"2022-01-29T11:14:15.909270Z","iopub.status.idle":"2022-01-29T11:14:15.915014Z","shell.execute_reply.started":"2022-01-29T11:14:15.909240Z","shell.execute_reply":"2022-01-29T11:14:15.913960Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\nX_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:14:37.839862Z","iopub.execute_input":"2022-01-29T11:14:37.840521Z","iopub.status.idle":"2022-01-29T11:14:37.844888Z","shell.execute_reply.started":"2022-01-29T11:14:37.840482Z","shell.execute_reply":"2022-01-29T11:14:37.844043Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator \ndatagen = ImageDataGenerator( \n    rescale=1./255,\n    rotation_range = 10,\n    horizontal_flip = True,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    fill_mode = 'nearest')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:14:39.087959Z","iopub.execute_input":"2022-01-29T11:14:39.088217Z","iopub.status.idle":"2022-01-29T11:14:39.093045Z","shell.execute_reply.started":"2022-01-29T11:14:39.088186Z","shell.execute_reply":"2022-01-29T11:14:39.092246Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"testgen = ImageDataGenerator(rescale=1./255)\ndatagen.fit(X_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:14:47.989275Z","iopub.execute_input":"2022-01-29T11:14:47.989524Z","iopub.status.idle":"2022-01-29T11:14:48.174187Z","shell.execute_reply.started":"2022-01-29T11:14:47.989496Z","shell.execute_reply":"2022-01-29T11:14:48.173467Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:14:52.793109Z","iopub.execute_input":"2022-01-29T11:14:52.793786Z","iopub.status.idle":"2022-01-29T11:14:52.797584Z","shell.execute_reply.started":"2022-01-29T11:14:52.793742Z","shell.execute_reply":"2022-01-29T11:14:52.796572Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_flow = datagen.flow(X_train, y_train, batch_size=batch_size) \ntest_flow = testgen.flow(X_test, y_test, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:17:19.282987Z","iopub.execute_input":"2022-01-29T11:17:19.283747Z","iopub.status.idle":"2022-01-29T11:17:19.356925Z","shell.execute_reply.started":"2022-01-29T11:17:19.283690Z","shell.execute_reply":"2022-01-29T11:17:19.356179Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom tensorflow.keras.optimizers import Adam , SGD\nfrom keras.regularizers import l1, l2\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:17:20.714300Z","iopub.execute_input":"2022-01-29T11:17:20.714548Z","iopub.status.idle":"2022-01-29T11:17:20.719912Z","shell.execute_reply.started":"2022-01-29T11:17:20.714520Z","shell.execute_reply":"2022-01-29T11:17:20.719232Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def FER_Model(input_shape=(48,48,1)):\n    # first input model\n    visible = Input(shape=input_shape, name='input')\n    num_classes = 7\n    #the 1-st block\n    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n    conv1_1 = BatchNormalization()(conv1_1)\n    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n    conv1_2 = BatchNormalization()(conv1_2)\n    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)#the 2-nd block\n    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n    conv2_1 = BatchNormalization()(conv2_1)\n    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n    conv2_2 = BatchNormalization()(conv2_2)\n    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n    conv2_2 = BatchNormalization()(conv2_3)\n    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)#the 3-rd block\n    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n    conv3_1 = BatchNormalization()(conv3_1)\n    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n    conv3_2 = BatchNormalization()(conv3_2)\n    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n    conv3_3 = BatchNormalization()(conv3_3)\n    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n    conv3_4 = BatchNormalization()(conv3_4)\n    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)#the 4-th block\n    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n    conv4_1 = BatchNormalization()(conv4_1)\n    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n    conv4_2 = BatchNormalization()(conv4_2)\n    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n    conv4_3 = BatchNormalization()(conv4_3)\n    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n    conv4_4 = BatchNormalization()(conv4_4)\n    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n    \n    #the 5-th block\n    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n    conv5_1 = BatchNormalization()(conv5_1)\n    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n    conv5_2 = BatchNormalization()(conv5_2)\n    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n    conv5_3 = BatchNormalization()(conv5_3)\n    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n    conv5_3 = BatchNormalization()(conv5_3)\n    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)#Flatten and output\n    flatten = Flatten(name = 'flatten')(drop5_1)\n    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model \n    model = Model(inputs =visible, outputs = ouput)\n    # summary layers\n    print(model.summary())\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:17:21.102772Z","iopub.execute_input":"2022-01-29T11:17:21.103193Z","iopub.status.idle":"2022-01-29T11:17:21.124478Z","shell.execute_reply.started":"2022-01-29T11:17:21.103161Z","shell.execute_reply":"2022-01-29T11:17:21.122001Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model = FER_Model()\nopt = Adam(lr=0.0001, decay=1e-6)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:17:22.292228Z","iopub.execute_input":"2022-01-29T11:17:22.292474Z","iopub.status.idle":"2022-01-29T11:17:22.576983Z","shell.execute_reply.started":"2022-01-29T11:17:22.292445Z","shell.execute_reply":"2022-01-29T11:17:22.576289Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100  \nhistory = model.fit_generator(train_flow, \n                    steps_per_epoch=len(X_train) / batch_size, \n                    epochs=num_epochs,  \n                    verbose=1,  \n                    validation_data=test_flow,\n                             )","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:17:34.987461Z","iopub.execute_input":"2022-01-29T11:17:34.987904Z","iopub.status.idle":"2022-01-29T11:49:25.801788Z","shell.execute_reply.started":"2022-01-29T11:17:34.987868Z","shell.execute_reply":"2022-01-29T11:49:25.801085Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:49:49.895503Z","iopub.execute_input":"2022-01-29T11:49:49.896188Z","iopub.status.idle":"2022-01-29T11:49:50.055500Z","shell.execute_reply.started":"2022-01-29T11:49:49.896130Z","shell.execute_reply":"2022-01-29T11:49:50.054782Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tensorflow.keras.models import load_model, model_from_json\nfrom tensorflow.keras.preprocessing.image import load_img,img_to_array\nfrom tensorflow.keras.preprocessing import image\nmodel = model_from_json(open(\"./model.json\", \"r\").read())\nmodel.load_weights('./model.h5')\n# model = load_model('static\\Fer2013.h5')\nface_haar_cascade = cv2.CascadeClassifier('static\\haarcascade_frontalface_default.xml')\ncap=cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    res,frame=cap.read()\n\n    height, width , channel = frame.shape\n    sub_img = frame[0:int(height/6),0:int(width)]\n\n    black_rect = np.ones(sub_img.shape, dtype=np.uint8)*0\n    res = cv2.addWeighted(sub_img, 0.77, black_rect,0.23, 0)\n    FONT = cv2.FONT_HERSHEY_SIMPLEX\n    FONT_SCALE = 0.8\n    FONT_THICKNESS = 2\n    lable_color = (10, 10, 255)\n    lable = \"Emotion Detection made by Abhishek\"\n    lable_dimension = cv2.getTextSize(lable,FONT ,FONT_SCALE,FONT_THICKNESS)[0]\n    textX = int((res.shape[1] - lable_dimension[0]) / 2)\n    textY = int((res.shape[0] + lable_dimension[1]) / 2)\n    cv2.putText(res, lable, (textX,textY), FONT, FONT_SCALE, (0,0,0), FONT_THICKNESS)\n    gray_image= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    faces = face_haar_cascade.detectMultiScale(gray_image )\n    try:\n        for (x,y, w, h) in faces:\n            cv2.rectangle(frame, pt1 = (x,y),pt2 = (x+w, y+h), color = (255,0,0),thickness =  2)\n            roi_gray = gray_image[y-5:y+h+5,x-5:x+w+5]\n            roi_gray=cv2.resize(roi_gray,(48,48))\n            image_pixels = img_to_array(roi_gray)\n            image_pixels = np.expand_dims(image_pixels, axis = 0)\n            image_pixels /= 255\n            predictions = model.predict(image_pixels)\n            max_index = np.argmax(predictions[0])\n            emotion_detection = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n            emotion_prediction = emotion_detection[max_index]\n            cv2.putText(res, \"Sentiment: {}\".format(emotion_prediction), (0,textY+22+5), FONT,0.7, lable_color,2)\n            lable_violation = 'Confidence: {}'.format(str(np.round(np.max(predictions[0])*100,1))+ \"%\")\n            violation_text_dimension = cv2.getTextSize(lable_violation,FONT,FONT_SCALE,FONT_THICKNESS )[0]\n            violation_x_axis = int(res.shape[1]- violation_text_dimension[0])\n            cv2.putText(res, lable_violation, (violation_x_axis,textY+22+5), FONT,0.7, lable_color,2)\n    except :\n        pass\n    frame[0:int(height/6),0:int(width)] =res\n    cv2.imshow('frame', frame)\n\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n\n\ncap.release()\ncv2.destroyAllWindows\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:57:10.416964Z","iopub.execute_input":"2022-01-29T11:57:10.417368Z","iopub.status.idle":"2022-01-29T11:57:10.791631Z","shell.execute_reply.started":"2022-01-29T11:57:10.417331Z","shell.execute_reply":"2022-01-29T11:57:10.790987Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}